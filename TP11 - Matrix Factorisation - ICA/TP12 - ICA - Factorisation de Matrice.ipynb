{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA,ICA,NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SÉPARATION DE SOURCES AUDIO PAR ICA -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se propose dans cette première partie d’utiliser l’ICA pour séparer les deux sources audio (monophoniques) $s_k(n)$, $k \\in \\{1, 2\\}$, qui composent un mélange stéréophonique $x_l(n)$, $l \\in \\{1, 2\\}$, dit *linéaire instantané*, c’est-à-dire obtenu par combinaisons linéaires des sources\n",
    "monophoniques suivant :\n",
    "\n",
    "$$x_l(n) = \\sum_{k=1}^2 a_{l,k}s_k(n)$$\n",
    "\n",
    "les coefficients $a_{l,k}$ étant les gains de mixage, autrement dit les intensités avec lesquelles les\n",
    "sources contribuent à chaque mélange.\n",
    "1. Étudier le script **ica_audio.py** et l’utiliser pour générer le mélange stéréo. Prendre le\n",
    "temps d’écouter les différents signaux.\n",
    "2. Utiliser l’ICA pour extraire les sources originales à partir du mélange. On exploitera pour cela **sklearn.decomposition.FastICA.**\n",
    "3. Créer les fichiers audio pour chaque source extraite et les écouter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - EXTRACTION DE CARACTÉRISTIQUES DE VISAGES -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On étudie ici l’utilisation des techniques de décomposition par PCA et NMF pour l’extraction de caractéristiques utiles à la reconnaissance automatique de visages.\n",
    "\n",
    "1. Étudier et tester le script **pca_nmf_faces.py**. Analyser le type de décomposition obtenu par NMF en comparaison avec celui obtenu par PCA.\n",
    "\n",
    "\n",
    "2. Modifier le script pour réaliser une évaluation des performances d’un système de reconnaissance automatique de visages utilisant les caractéristiques extraites par PCA, comparées à celles obtenues par un système exploitant les caractéristiques extraites par NMF. On pourra utiliser la LDA pour la classification. On effectuera l’évaluation par validation croisée. On observera l’évolution des scores en faisant varier le nombre de composantes utilisé dans les décompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VERSION CORRIGE PAR ANTONIN\n",
    "\n",
    "Pour la partie \"Extraction de visage\" du TP, le code fourni n'a pas l'air de marcher, en le modifiant comme ça ça a l'air de mieux fonctionner :\n",
    "\n",
    "il faut également \n",
    "- supprimer le paramètre hold=True  dans la fonction plot_gallery\n",
    "- Remplacer init=0 par init=None  dans la définition de estimators \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_nmf_faces.py\n",
    "\n",
    "# Authors: Vlad Niculae, Alexandre Gramfort, Slim Essid\n",
    "# License: BSD\n",
    "\n",
    "from time import time\n",
    "from numpy.random import RandomState\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn import decomposition\n",
    "\n",
    "# -- Prepare data and define utility functions ---------------------------------\n",
    "\n",
    "n_row, n_col = 2, 5\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "\n",
    "faces_centered = faces - faces.mean(axis=0, dtype=np.float64)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "def plot_gallery(title, images):\n",
    "    pl.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    pl.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        \n",
    "        comp = comp.reshape(image_shape)\n",
    "        vmax = comp.max()\n",
    "        vmin = comp.min()\n",
    "        dmy = np.nonzero(comp<0)\n",
    "        if len(dmy[0])>0:\n",
    "            yz, xz = dmy\n",
    "        comp[comp<0] = 0\n",
    "       \n",
    "        pl.imshow(comp, cmap=pl.cm.gray, vmax=vmax, vmin=vmin)\n",
    "        #print \"vmax: %f, vmin: %f\" % (vmax, vmin)\n",
    "        #print comp\n",
    "      \n",
    "        if len(dmy[0])>0:\n",
    "            pl.plot( xz, yz, 'r,')    #, hold=True)\n",
    "            print(len(dmy[0]), \"negative-valued pixels\")\n",
    "            \n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "        \n",
    "    pl.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "    \n",
    "# Plot a sample of the input data\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "# -- Decomposition methods -----------------------------------------------------\n",
    "\n",
    "# List of the different estimators and whether to center the data\n",
    "\n",
    "estimators = [\n",
    "    ('pca', 'Eigenfaces - PCA',\n",
    "     decomposition.PCA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "  \n",
    "    ('nmf', 'Non-negative components - NMF',\n",
    "     decomposition.NMF(n_components=n_components, init=None, tol=1e-6,\n",
    "                       max_iter=2000),\n",
    "     False)      \n",
    "]\n",
    "\n",
    "# -- Transform and classify ----------------------------------------------------\n",
    "\n",
    "labels = dataset.target\n",
    "X = faces\n",
    "X_ = faces_centered\n",
    "\n",
    "for shortname, name, estimator, center in estimators:\n",
    "    #if shortname != 'nmf': continue\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time()\n",
    "\n",
    "    data = X\n",
    "    if center:\n",
    "        data = X_\n",
    "   \n",
    "    data = estimator.fit_transform(data)\n",
    "    \n",
    "    train_time = (time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    \n",
    "    components_ = estimator.components_\n",
    "    \n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                     components_[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - EXTRACTION DE THÈMES À PARTIR DE TEXTES -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s’agit dans cette partie de tester l’utilisation de la NMF pour l’extraction de thèmes à partir d’un corpus de textes ; l’idée principale étant d’interpréter chaque composante NMF extraite comme étant associée à un thème.\n",
    "\n",
    "Étudier et tester le script suivant (introduit sur http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9ce2c1d38836>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9ce2c1d38836>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    print \"done in %0.3fs.\" % (time() - t0)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it using the most common word\n",
    "# frequency with TF-IDF weighting (without top 5% stop words)\n",
    "\n",
    "t0 = time()\n",
    "print(\"Loading dataset and extracting TF-IDF features...\")\n",
    "dataset = datasets.fetch_20newsgroups(shuffle=True, random_state=1)\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_df=0.95, max_features=n_features)\n",
    "counts = vectorizer.fit_transform(dataset.data[:n_samples])\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)\n",
    "print \"done in %0.3fs.\" % (time() - t0)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model on with n_samples=%d and n_features=%d...\" % (\n",
    "n_samples, n_features))\n",
    "nmf = decomposition.NMF(n_components=n_topics).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
